## 第二篇：深入解析高性能服务器编程

### 第五章：linux网络编程基础API

1. socket地址api：

   1. 主机序(小端)和网络序(大端)
   2. 通用sockaddr   结构体sockaddr
   3. 专用sockaddr   结构体sockaddr_un sockaddr_in sockaddr_in6
   4. IP地址转换函数

2. 创建socket、命名、监听: create bind listen

3. 连接相关：接受、发起、关闭 accept 

4. 数据读写 send recv

   上面三个里面涉及很多的标志位，能够产生不同的效果

5. 带外标记

6. 地址信息函数

7. scoket选项 setsockopt getsockopt 选项对应的功能比较重要
   ![image-20210308192134473](https://github.com/zb1997/mynote/blob/main/tupian/image-20210308192134473.png)

8. 网络信息api

### 第六章：高级IO函数：

​	分为三类：	

​		用于创建文件描述符的函数，包括pip、dup、dup2  

​		用于读写数据：readv、writev、sendfile、mmap、munmap、splice、tee

​		用于控制IO行为和属性：fcntl

1. pip函数：用于创建一个管道，实现进程间通信

   1. 管道内部传输的数据是字节流， 这和TCP字节流的概念相同。 但二者又有细微的区别。 应用层程序能往一个TCP连接中写入多少字节的数据， 取决于对方的接收通告窗口的大小和本端的拥塞窗口的大小。 而管道本身拥有一个容量限制， 它规定如果应用程序不将数据从管道读走的话， 该管道最多能被写入多少字节的数据。 自Linux 2.6.11内核起，管道容量的大小默认是65536字节。 我们可以使用fcntl函数来修改管道容量  

2. dup函数：创建一个新的文件描述符，他和原有描述符只想相同的文件、管道或者网络连接，并且dup返回的文件描述符总是取当前系统当前可用的最小值。dup2和dup相似，但是他返回第一个不小于原描述符的整数值

3. readv和writev函数：readv函数将数据从文件描述符读到分散的内存块中，writev函数将多块分散的内存数据一并写入文件描述符中。分散读和集中写

4. sendfile函数：在两个描述符之间直接传递数据(完全在内核中操作)，从而避免了内核缓冲区和用户缓冲区之间的数据拷贝，效率很高，被称为零拷贝。

   该函数的man手册明确指出，in_fd必须是一个支持类似mmap函数的文件描述符，即他必须指向真实的文件，不能是socket或者管道。而out_fd一定是scoket。由此可见，sendfile几乎是专为在网络上传输文件为设计的

5. mmap和munmap函数：用于申请一段内存空间，我们可以将这段内存作为进程间通信的共享内存，而munmap函数则是释放这段内存空间。

6. splice函数：用于在两个文件描述符之间移动数据，也是零拷贝操作。使用splice函数时， fd_in和fd_out必须至少有一个是**管道**文件描述符  

7. tee函数：在两个管道文件描述符之间复制数据，也是零拷贝操作。他不消耗数据，因此源文件描述符上为数据仍然可以用于后续的读操作

8. fcntl函数：filecontrol，提供了对文件描述符的各种控制操作。另外一个常见的控制文件描述符属性和行为的系统调用是ioctl。而且ioctl比fcntl能够执行更多的控制，但是对于文件描述符常用的属性和行为，fcntl函数是由POSIX规范制定的首选方法。在网络编程中，fnctl经常被用来设置非阻塞标志

### 第七章：linux服务器程序规范：

- linux服务器程序一般以守护进程形式运行，没有控制终端，因而也不会意外收到用户输入，守护进程的父进程通常是init进程(PID = 1)
- Linux服务器程序通常有一套日志系统， 它至少能输出日志到文件， 有的高级服务器还能输出日志到专门的UDP服务器。 大部分后台进程都在/var/log目录下拥有自己的日志目录。
- Linux服务器程序一般以某个专门的非root身份运行。 比如mysqld、 httpd、 syslogd等后台进程， 分别拥有自己的运行账户mysql、 apache和syslog。
- Linux服务器程序通常是可配置的。 服务器程序通常能处理很多命令行选项， 如果一次运行的选项太多， 则可以用配置文件来管理。 绝大多数服务器程序都有配置文件， 并存放在/etc目录下。 比如第4章讨论的squid服务器的配置文件是/etc/squid3/squid.conf。
- Linux服务器进程通常会在启动的时候生成一个PID文件并存入/var/run目录中， 以记录该后台进程的PID。 比如syslogd的PID文件是/var/run/syslogd.pid  
- Linux服务器程序通常需要考虑系统资源和限制， 以预测自身能承受多大负荷， 比如进程可用文件描述符总数和内存总量等。  

1. 日志
   1. linux系统日志：linux提供一个守护进程来处理系统日志--rsyslogd
      1. rsyslogd守护进程既能接收用户进程输出的日志，也能接收内内核日志。用户进程通过调用syslog函数生成系统日志，该函数将日志输出到一个UNIX本地域socket类型的文件/dev/log中，rsyslogd则监听该文件获取进程的输出。
      2. 内核日志在老的系统上是通过另外一个守护进程rklogd来管理的， rsyslogd利用额外的模块实现了相同的功能。 内核日志由printk等函数打印至内核的环状缓存（ringbuffer） 中。 环状缓存的内容直接映射到/proc/kmsg文件中。 rsyslogd则通过读取该文件获得内核日志。  
      3. rsyslogd守护进程在接收到用户进程或内核输入的日志后， 会把它们输出至某些特定的日志文件。 默认情况下， 调试信息会保存至/var/log/debug文件， 普通信息保存至/var/log/messages文件， 内核消息则保存至/var/log/kern.log文件。 不过， 日志信息具体如何分发， 可以在rsyslogd的配置文件中设置。 rsyslogd的主配置文件是/etc/rsyslog.conf， 其中主要可以设置的项包括： 内核日志输入路径， 是否接收UDP日志及其监听端口（默认是514， 见/etc/services文件） ， 是否接收TCP日志及其监听端口， 日志文件的权限， 包含哪些子配置文件（比如/etc/rsyslog.d/*.conf） 。 rsyslogd的子配置文件则指定各类日志的目标存储文件  
      4. ![image-20210308205139378](https://github.com/zb1997/mynote/blob/main/tupian/image-20210308205139378.png)
   2. syslog函数:日志掩码
2. 用户信息
   1. 真实用户UID、有效用户EUID、真实组GID、有效组EGID
      1. EUID存在的目的是方便用来进行资源访问，使得运行程序的用户拥有该程序的有效权限。EGID也是一样的意义
   2. 切换用户
3. 进程间关系
   1. 进程组：linux每个进程都属于一个进程组，因此他们除了PID外，还有进程组ID。一个进程只能对设置自己或者子进程的PGID
   2. 会话session
   3. 用ps命令查看进程关系：
4. 系统资源限制：linux上运行的程序都会受到资源限制的影响，比如物理设备、系统策略、和具体实现的限制。
5. 改变工作目录和根目录
6. 服务器程序后台化：守护进程![image-20210310200127158](https://github.com/zb1997/mynote/blob/main/tupian/image-20210310200127158.png)

### 第八章：高性能服务器程序框架

**全书的核心**

将服务器解构为如下三个主要模块：

- **IO处理单元**：介绍IO处理单元的四种IO模型和两种高效事件处理模式
- **逻辑单元**：介绍逻辑单元两种高效并发模式，以及高效的逻辑处理方式：有限状态机
- 存储单元

1. ##### 服务器模型：

   1. ###### **C/S模型**：服务器端一般通过IO模型监听事件请求，在监听到连接请求之后，服务器调用accept函数接受它，并分配一个逻辑单元为新的连接服务。逻辑单元可以是新创建的子进程、子线程、或者其他。

   2. **P2P模型**：摒弃了以服务器为中心的格局，让网络所有主机重新回归对等的地位，使得每台机器都在消耗服务的同时也为其他人提供服务。云计算机群可以看做P2P模型的一个典范。缺点很明显，当用户之间传输的请求过多时，网络的负载将加重，并且主机之间很难互相发现。

2. **服务器编程框架**:

   1. ![image-20210310202814160](https://github.com/zb1997/mynote/blob/main/tupian/image-20210310202814160.png)
   2. 注：请求队列是各单元之间的通信方式的抽象。 I/O处理单元接收到客户请求时， 需要以某种方式通知一个逻辑单元来处理该请求。 同样， 多个逻辑单元同时访问一个存储单元时， 也需要采用某种机制来协调处理竞态条件。 请求队列通常被实现为池的一部分， 我们将在后面讨论池的概念。 对于服务器机群而言， **请求队列是各台服务器之间预先建立的、 静态的、 永久的TCP连接**。 这种TCP连接能提高服务器之间交换数据的效率， 因为它避免了动态建立TCP连接导致的额外的系统开销。  

3. **IO模型**：

   1. **阻塞IO和非阻塞IO**：可能被阻塞的系统调用有：accept、send、recv、connect
      1. 针对非阻塞I/O执行的系统调用则总是立即返回， 而不管事件是否已经发生。 如果事件没有立即发生，这些系统调用就返回-1， 和出错的情况一样。 此时我们必须根据errno来区分这两种情况。对accept、 send和recv而言， 事件未发生时errno通常被设置成EAGAIN（意为“再来一次”） 或者EWOULDBLOCK（意为“期望阻塞”） ； 对connect而言， errno则被设置成EINPROGRESS（意为“在处理中 ")
      2. 非阻塞I/O通常要和其他I/O通知机制一起使用， 比如**I/O复用和SIGIO信号**。 
         1.  **I/O复用**是最常使用的I/O通知机制。 它指的是， 应用程序通过I/O复用函数向内核注册一组事件， 内核通过I/O复用函数把其中就绪的事件通知给应用程序。 Linux上常用的I/O复用函数是select、 poll和epoll_wait，需要指出的是， **I/O复用函数本身是阻塞的**， 它们能提高程序效率的原因在于它们具有同时监听多个I/O事件的能力。  
         2.  SIGIO信号也可以用来报告I/O事件。 我们可以为一个目标文件描述符指定宿主进程， 那么被指定的宿主进程将捕获到SIGIO信号。 这样， 当目标文件描述符上有事件发生时， SIGIO信号的信号处理函数将被触发， 我们也就可以在该信号处理函数中对目标文件描述符执行非阻塞I/O操作了。 
   2. 从理论上说， **阻塞I/O、 I/O复用和信号驱动I/O都是同步I/O模型**。 因为在这三种I/O模型中， I/O的读写操作， 都是在I/O事件发生之后， 由应用程序来完成的。 而POSIX规范所定义的**异步I/O模型**则不同。 对异步I/O而言， 用户可以直接对I/O执行读写操作， 这些操作告诉内核用户读写缓冲区的位置， 以及I/O操作完成之后内核通知应用程序的方式。 异步I/O的读写操作总是立即返回， 而不论I/O是否是阻塞的， 因为真正的读写操作已经由内核接管。 也就是说， **同步I/O模型要求用户代码自行执行I/O操作（将数据从内核缓冲区读入用户缓冲区， 或将数据从用户缓冲区写入内核缓冲区） ， 而异步I/O机制则由内核来执行I/O操作（数据在内核缓冲区 和用户缓冲区之间的移动是由内核在“后台”完成的）** 。 你可以这样认为， **同步I/O向应用程序通知的是I/O就绪事件， 而异步I/O向应用程序通知的是I/O完成事件**。 Linux环境下， aio.h头文件中定义的函数提供了对异步I/O的支持。   
   3. ![image-20210310210717168](https://github.com/zb1997/mynote/blob/main/tupian/image-20210310210717168.png)

4. 两种高效的**事件处理模式**：同步IO模型通常用于实现REACTOR模式，异步IO则用于实现PROACTOR

   1. **REACTOR**：

      1. > Reactor是这样一种模式， 它要求主线程（I/O处理单元， 下同） 只负责监听文件描述上是否有事件发生， 有的话就立即将该事件通知工作线程（逻辑单元， 下同） 。 除此之外， 主线程不做任何其他实质性的工作。 读写数据， 接受新的连接， 以及处理客户请求均在工作线程中完成。
         > 使用同步I/O模型（以epoll_wait为例） 实现的Reactor模式的工作流程是：
         > 1） 主线程往epoll内核事件表中注册socket上的读就绪事件。
         > 2） 主线程调用epoll_wait等待socket上有数据可读。
         > 3） 当socket上有数据可读时， epoll_wait通知主线程。 主线程则将socket可读事件放入请求队列。
         > 4） 睡眠在请求队列上的某个工作线程被唤醒， 它从socket读取数据， 并处理客户请求， 然后往epoll内核事件表中注册该socket上的写就绪事件。
         > 5） 主线程调用epoll_wait等待socket可写。
         > 6） 当socket可写时， epoll_wait通知主线程。 主线程将socket可写事件放入请求队列。
         > 7） 睡眠在请求队列上的某个工作线程被唤醒， 它往socket上写入服务器处理客户请求的结果。  

      2. ![image-20210311192641675](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311192641675.png)
         工作线程从请求队列中取出事件后， 将根据事件的类型来决定如何处理它： 对于可读事件，执行读数据和处理请求的操作； 对于可写事件， 执行写数据的操作。 因此， 图8-5所示的Reactor模式中， 没必要区分所谓的“读工作线程”和“写工作线程”。  

   2. **PROACTOR**：

      1. > 与Reactor模式不同， Proactor模式将所有I/O操作都交给主线程和内核来处理， 工作线程仅仅负责业务逻辑。 因此， Proactor模式更符合图8-4所描述的服务器编程框架。使用异步I/O模型（以aio_read和aio_write为例） 实现的Proactor模式的工作流程是：
         > 1） 主线程调用aio_read函数向内核注册socket上的读完成事件， 并告诉内核用户读缓冲区的位置， 以及读操作完成时如何通知应用程序（这里以信号为例， 详情请参考sigevent的man手册） 。
         > 2） 主线程继续处理其他逻辑。
         > 3） 当socket上的数据被读入用户缓冲区后， 内核将向应用程序发送一个信号， 以通知应用程序数据已经可用。
         > 4） 应用程序预先定义好的信号处理函数选择一个工作线程来处理客户请求。 工作线程处理完客户请求之后， 调用aio_write函数向内核注册socket上的写完成事件， 告诉内核用户写缓冲区的位置， 以及写操作完成时如何通知应用程序（仍然以信号为例） 。
         > 5） 主线程继续处理其他逻辑。
         > 6） 当用户缓冲区的数据被写入socket之后， 内核将向应用程序发送一个信号， 以通知应用程序数据已经发送完毕。
         > 7） 应用程序预先定义好的信号处理函数选择一个工作线程来做善后处理， 比如决定是否关闭socket。  

      2. ![image-20210311194012372](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311194012372.png)
         连接socket上的读写事件是通过aio_read/aio_write向内核注册的， 因此内核将通过信号来向应用程序报告连接socket上的读写事件。 所以， 主线程中的epoll_wait调用仅能用来检测监听socket上的连接请求事件， 而不能用来检测连接socket上的读写事件。  

      3. 模拟Proactor模式：使用同步IO方式模拟出PROACTOR模式。原理是:主线程执行数据读写操作，读写完成之后，主线程向工作线程通知这一完成事件，那么从工作线程的角度看，他们就直接获得了数据读写的结果，接下来要做的只是对读写结果进行逻辑处理

         > 使用同步I/O模型（仍然以epoll_wait为例） 模拟出的Proactor模式的工作流程如下：
         > 1） 主线程往epoll内核事件表中注册socket上的读就绪事件。
         > 2） 主线程调用epoll_wait等待socket上有数据可读。
         > 3） 当socket上有数据可读时， epoll_wait通知主线程。 主线程从socket循环读取数据， 直到没有更多数据可读， 然后将读取到的数据封装成一个请求对象并插入请求队列。
         > 4） 睡眠在请求队列上的某个工作线程被唤醒， 它获得请求对象并处理客户请求， 然后往epoll内核事件表中注册socket上的写就绪事件。
         > 5） 主线程调用epoll_wait等待socket可写。
         > 6） 当socket可写时， epoll_wait通知主线程。 主线程往socket上写入服务器处理客户请求的结果  

         ![image-20210311194439760](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311194439760.png)

5. 两种高效的**并发模式**：半同步/半异步模式(half-sync/half-async)和领导者/追随者模式(Leader/Followers)

   1. **半同步半异步模式**：

      1. 同步与异步:
         1. IO模型中的同步异步区分的是内核向应用程序通知的是何种IO事件(就绪或者完成)，以及谁来完成IO读写(应用程序或者是内核)。
         2. 并发模型中的同步是值程序完全按照代码序列的顺序执行，异步指的是程序的执行那个需要由系统事件来驱动。常见的系统事件包括终端、信号等
         3. ![image-20210311195348494](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311195352826.png)
         4. 同步并发模式：效率相对较低，实时性差，但是逻辑简单
            异步并发模式：效率高、实时性强、但是相对复杂，难于调试和扩展
      2. 变体：**半同步半反应堆模式**：![image-20210311200343287](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311200343287.png)
         1. 缺点：主线程和工作线程共享请求队列，读写都需要枷锁保护，白白消耗CPU时间
         2. 每个工作线程一个事件只能处理一个客户请求，如果客户数量很多，客户的响应速度也会变慢，如果增加工作线程，则工作线程的切换也会耗费大量的CPU时间
      3. 相对高效的半同步半异步模式：![image-20210311200840960](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311200840960.png)
         1. 主线程只管监听socket，连接socket由工作线程来管理。当有新的连接到来，主线程就接收并将新返回的socket派发给某个工作线程，此后的所有操作都归工作线程管理。主线程向工作线程派发scoket最简单的方式是往他和工作线程之间的管道写数据，工作线程检测到管道中的数据。

   2. **领导者/追随者模式**：

      1. 多个工作线程轮流获得事件源集合， 轮流监听、 分发并处理事件的一种模式。 在任意时间点，程序都仅有一个领导者线程， 它负责监听I/O事件。 而其他线程则都是追随者， 它们休眠在线程池中等待成为新的领导者。 当前的领导者如果检测到I/O事件， 首先要从线程池中推选出新的领导者线程， 然后处理I/O事件。 此时， 新的领导者等待新的I/O事件， 而原来的领导者则处理I/O事件， 二者实现了并发。  

      2. 包含组件：句柄集、线程集、事件处理器和具体事件处理器![image-20210311201656702](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311201656702.png)

         1. 句柄集：表示I/O资源， 在Linux下通常就是一个文件描述符。 句柄集管理众多句柄， 它使用
            wait_for_event方法来监听这些句柄上的I/O事件， 并将其中的就绪事件通知给领导者线程。 领导者则调用绑定到Handle上的事件处理器来处理事件。 领导者将Handle和事件处理器绑定是通过调用句柄集中的register_handle方法实现的。  

         2. 线程集：是所有工作线程（包括领导者线程和追随者线程） 的管理者。 它负责各线程之间的同步， 以及新领导者线程的推选。 线程集中的线程在任一时间必处于如下三种状态之一：

            - Leader： 线程当前处于领导者身份， 负责等待句柄集上的I/O事件。

            - Processing： 线程正在处理事件。 领导者检测到I/O事件之后， 可以转移到Processing状态来处理该事件， 并调用promote_new_leader方法推选新的领导者； 也可以指定其他追随者来处理事件（EventHandoff） ， 此时领导者的地位不变。 当处于Processing状态的线程处理完事件之后， 如果当前线程集中没有领导者， 则它将成为新的领导者， 否则它就直接转变为追随者。

            - Follower： 线程当前处于追随者身份， 通过调用线程集的join方法等待成为新的领导者， 也可能被当前的领导者指定来处理新的任务  

              状态转换关系：

              ![image-20210311202637060](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311202637060.png)

         3. 事件处理器和具体事件处理器

            1. 通常包含一个或多个回调函数handle_event。 这些回调函数用于处理事件对应的业务逻辑。事件处理器在使用前需要被绑定到某个句柄上， 当该句柄上有事件发生时， 领导者就执行与之绑定的事件处理器中的回调函数。  
            2. 工作流程：![image-20210311203049785](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311203049785.png)

6. **有限状态机**：逻辑单元内的高效编程方法

7. 提高服务器性能的其他建议：

   1. 池：由于服务器基本上硬件资源充足，所以可以使用以空间换时间的方案，这就是此的概念。池是一组资源的集合，在服务器启动的时候就静态分配好，无需动态分配。释放时只需要将资源还给池中，无需执行释放操作。可以避免很多服务器对内核的操作

      1. 两种分配方案：分配足够多的资源和预分配一定数量的资源，之后发现不够在进行动态分配
      2. 内存池 进程池 线程池 连接池

   2. 数据复制：避免不必要的数据复制尤其是当数据复制发生在用户代码和内核之间的时候。 如
      果内核可以直接处理从socket或者文件读入的数据， 则应用程序就没必要将这些数据从内核缓冲区制到应用程序缓冲区中。 这里说的“直接处理”指的是应用程序不关心这些数据的内容， 不需要对它们做任何分析。 比如ftp服务器， 当客户请求一个文件时， 服务器只需要检测目标文件是否存在， 以及客户是否有读取它的权限， 而绝对不会关心文件的具体内容。 这样的话， **ftp服务器就无须把目标文件的内容完整地读入到应用程序缓冲区中并调用send函数来发送， 而是可以使用“零拷贝”函数sendfile来直接将其发送给客户端**  

   3. 上下文切换和锁：

      > 并发程序必须考虑上下文切换（context switch） 的问题， 即进程切换或线程切换导致的的系统开销。即使是I/O密集型的服务器， 也不应该使用过多的工作线程（或工作进程， 下同） ， 否则线程间的切换将占用大量的CPU时间， 服务器真正用于处理业务逻辑的CPU时间的比重就显得不足了。 因此， 为每个客户连接都创建一个工作线程的服务器模型是不可取的。 图8-11所描述的半同步/半异步模式是一种比较合理的解决方案， 它允许一个线程同时处理多个客户连接。 此外， 多线程服务器的一个优点是不同的线程可以同时运行在不同的CPU上。 当线程的数量不大于CPU的数目时， 上下文的切换就不是问题了。
      > 并发程序需要考虑的另外一个问题是共享资源的加锁保护。 锁通常被认为是导致服务器效率低下的一个因素， 因为由它引入的代码不仅不处理任何业务逻辑， 而且需要访问内核资源。 因此， 服务器如果有更好的解决方案， 就应该避免使用锁。 显然， 图8-11所描述的半同步/半异步模式就比图8-10所描述的半同步/半反应堆模式的效率高。 如果服务器必须使用“锁”， 则可以考虑减小锁的粒度， 比如使用读写锁。 当所有工作线程都只读取一块共享内存的内容时， 读写锁并不会增加系统的额外开销。 只有当其中某一个工作线程需要写这块内存时， 系统才必须去锁住这块区域。  

### 第九章：IO复用

I/O复用虽然能同时监听多个文件描述符， 但它本身是阻塞的。 并且当多个文件描述符同时就绪时， 如果不采取额外的措施， 程序就只能按顺序依次处理其中的每一个文件描述符， 这使得服务器程序看起来像是串行工作的。 如果要实现并发， 只能使用多进程或多线程等编程手段  

1. select系统调用：采用轮询的方式对监管的socket进行监听
   1. select API
   2. 文件描述符就绪条件
      1. 可读：
         1. socket内核接收缓存区中的字节数大于或等于其低水位标记SO_RCVLOWAT。 此时我们可以无阻塞地读该socket， 并且读操作返回的字节数大于0。
         2. socket通信的对方关闭连接。 此时对该socket的读操作将返回0。
         3. 监听socket上有新的连接请求。
         4. socket上有未处理的错误。 此时我们可以使用getsockopt来读取和清除该错误。  
      2. 可写：
         1. socket内核发送缓存区中的可用字节数大于或等于其低水位标记SO_SNDLOWAT。 此时我们可以无阻塞地写该socket， 并且写操作返回的字节数大于0。
         2. socket的写操作被关闭。 对写操作被关闭的socket执行写操作将触发一个SIGPIPE信号。
         3. socket使用非阻塞connect连接成功或者失败（超时） 之后。
         4. socket上有未处理的错误。 此时我们可以使用getsockopt来读取和清除该错误。  
   3. 处理带外数据：scoket上接收到普通数据和带外数据都会返回，但是socket处于不同的就绪状态：前者处于可读状态，后者处于异常状态
2. poll系统调用：轮询
3. epoll系列系统调用：是linux特有的IO复用函数，他在实现和使用上与前两者有很大差异。epoll把用户关心的文件描述符放到内核的一个事件表中，无须每次调用都重复传入文件描述符集或事件集，但是epoll需要一个额外的文件描述符，来唯一标识内核的这个时间表。就是使用epoll_create创建的。
   1. 内核事件表
   2. epoll_wait函数
   3. **LT水平出发模式和ET边沿触发模式**：
      1. LT模式是默认的工作模式，相当于一个高效的poll。当epoll_wait检测到上面有事件发生并将此事件通知应用程序后，应用程序可以不立即处理，下一次epoll_wait的时候，epoll还会想程序通告此事件
      2. ET模式通过注册时使用EPOLLET，ET模式是epoll的高效工作模式，当事件通知应用程序后，应用程序必须立即处理，后续epoll_wait不会再继续上报此事件，ET模式在很大程度上降低了一个事件被重复触发的次数，因此效率要比LT模式高。注：每个使用ET模式的描述符都应该是非阻塞的
   4. **EPOLLONESHOT事件**：可以**“锁定”**某个正在被处理的socket，让其他的线程同一时间不能操作他。即使我们使用ET模式， 一个socket上的某个事件还是可能被触发多次。 这在并发程序中就会引起一个问题。 比如一个线程（或进程， 下同） 在读取完某个socket上的数据后开始处理这些数据， 而在数据的处理过程中该socket上又有新数据可读（EPOLLIN再次被触发） ， 此时另外一个线程被唤醒来读取这些新的数据。于是就出现了两个线程同时操作一个socket的局面。 这当然不是我们期望的。 我们期望的是**一个socket连接在任一时刻都只被一个线程处理**。 这一点可以使用epoll的EPOLLONESHOT事件实现。 
       对于注册了EPOLLONESHOT事件的文件描述符， 操作系统最多触发其上注册的一个可读、 可写或者异常事件， 且只触发一次， 除非我们使用epoll_ctl函数重置该文件描述符上注册的EPOLLONESHOT事件。 这样， 当一个线程在处理某个socket时， 其他线程是不可能有机会操作该socket的。 但反过来思考， 注册了EPOLLONESHOT事件的socket一旦被某个线程处理完毕， 该线程就应该立即重置这个socket上的EPOLLONESHOT事件， 以确保这个socket下一次可读时， 其EPOLLIN事件能被触发， 进而让其他工作线程有机会继续处理这个socket  
4. ![image-20210313145616323](https://github.com/zb1997/mynote/blob/main/tupian/image-20210313145616323.png)
5. IO复用的高级应用一：非阻塞connect：connect出错时的一种errno值： EINPROGRESS。 这种错误发生在对非阻塞的socket调用connect， 而连接又没有立即建立时。 根据man文档的解释， 在这种情况下， 我们可以调用select、 poll等函数来监听这个连接失败的socket上的可写事件。 当select、 poll等函数返回后， 再利用getsockopt来读取错误码并清除该socket上的错误。 如果错误码是0， 表示连接成功建立， 否则连接失败。  
6. 超级服务xinetd：xinetd是LINUX上的因特网服务。管理的子服务有的是标准服务，如事件日期服务daytime、echo服务、和丢弃服务discard。xinetd服务在内部直接处理这些服务。有的子服务需要调用外部程序来处理，如telnet和ftp服务。工作流程如下：![image-20210313151011004](https://github.com/zb1997/mynote/blob/main/tupian/image-20210313151011004.png)

### 第十章：信号



