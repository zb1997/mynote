## 第一篇：TCP/IP协议详解

### 第一章：TCP/IP协议族

1. 体系结构和主要结构：
   1. 数据链路层：实现网卡接口的网络驱动程序，处理数据在物理媒介上的传输。协议有ARP和RARP。在网络层使用IP寻址，链路层使用MAC地址寻址。RARP协议一般是使用在没有存储设备的机器上，他们利用网卡上的物理地址向网络管理者(通常存有所有机器的IP地址映射)查询自身ip
   2. 网络层：实现数据包的选路和转发。最主要的协议是IP协议和ICMP协议，ICMP协议是IP协议的重要补充，主要用与检测网络连接。注意ICMP协议不是严格意义上的网络层协议，因为它使用同层的IP协议提供服务
   3. 传输层：为两台主机上的应用层提供端到端的通信。他为上层的应用提供一条端到端的通信链路,并负责数据的可靠性等.主要的协议有TCP、UDP、SCTP
   4. 应用层：负责处理应用程序逻辑。链路层传输层负责通信的细节，这部分必须稳定高效。所以他们在内核空间中实现，而应用层在用户空间实现。也有少数的服务器程序是在内核中实现的，这样代码就没有必要在用户空间和内核中切换，提高工作效率。但是这种代码实现复杂，可移植性差。应用层的协议很多，如telnet、ftp、ospf 、dns等
   
2. 封装和分用：
   
   1. 封装：![image-20210306150718095](https://github.com/zb1997/mynote/blob/main/tupian/image-20210306150718095.png)
   2. 分用：![image-20210306150805808](https://github.com/zb1997/mynote/blob/main/tupian/image-20210306150805808.png)
   
3. ARP和DNS工作原理

4. socket和TCP/IP协议族的关系

   > 数据链路层、 网络层、 传输层协议是在内核中实现的。 因此操作系统需要实现一组系统调用， 使得应用程序能够访问这些协议提供的服务。 实现这组系统调用的API（Application Programming Interface， 应用程序编程接口） 主要有两套： socket和XTI。 XTI现在基本不再使用， 本书仅讨论socket。 图1-1显示了socket与TCP/IP协议族的关系。
   >
   > 由socket定义的这一组API提供如下两点功能： 一是将应用程序数据从用户缓冲区中复制到TCP/UDP内核发送缓冲区， 以交付内核来发送数据（比如图1-5所示的send函数） ， 或者是从内核TCP/UDP接收缓冲区中复制数据到用户缓冲区， 以读取数据； 二是应用程序可以通过它们来修改内核中各层协议的某些头部信息或其他数据结构， 从而精细地控制底层通信的行为。 比如可以通过setsockopt函数来设置IP数据报在网络上的存活时间。 我们将在第5章详细讨论这一组API。
   > 值得一提的是， socket是一套通用网络编程接口， 它不但可以访问内核中TCP/IP协议栈， 而且可以访问其他网络协议栈（比如X.25协议栈、 UNIX本地域协议栈等） 。  
   
   

### 第二章：IP协议详解

1. 特点：无状态、无连接、不可靠

   1. IP通信的双方不传输数据的状态，传输层控制状态。所以IP层简单高效，无需为保持通信的状态分配额外的资源
   2. 无连接指的是双方都不长久的维持对方的任何信息，上层协议每次发送数据的时候都需要明确指定对方的地址
   3. 不可靠值IP协议不保证IP数据包能够准确的到达接收端，只承诺最大努力交付

2. 结构：头部结构以及个字段含义：百度很清楚

   1. 松散路由选择(loose source routing)，只是给出IP数据报必须经过的一些"要点"，并不给出一条完备的路径，无直接连接的路由器之间的路由尚需IP软件的寻址
   2. 严格路由选择(strict source routing)，规定IP数据报要经过路径上的每一个路由器，相邻路由器之间不得有中间路由器，并且所经过路由器的顺序不可更改。

3. IP分片：当数据包的长度超过帧的MTU时，他将会被分片传输，分片可能发生在发送端，也可能会发生在中转路由器上，但是只有在目标机器上才会发生重组。IP头部有分片相关的字段

4. IP路由：

   1. IP模块工作流程![image-20210306152702658](https://github.com/zb1997/mynote/blob/main/tupian/image-20210306152702658.png)

      > 当IP模块接收到来自数据链路层的IP数据报时， 它首先对该数据报的头部做CRC校验， 确认无误之后就分析其头部的具体信息。
      > 如果该IP数据报的头部设置了源站选路选项（松散源路由选择或严格源路由选择） ， 则IP模块调用数据报转发子模块来处理该数据报。 如果该IP数据报的头部中目标IP地址是本机的某个IP地址， 或者是广播地址， 即该数据报是发送给本机的， 则IP模块就根据数据报头部中的协议字段来决定将它派发给哪个上层应用（分用） 。 如果IP模块发现这个数据报不是发送给本机的， 则也调用数据报转发子模块来处理该数据报。  数据报转发子模块将首先检测系统是否允许转发， 如果不允许， IP模块就将数据报丢弃。 如果允许，数据报转发子模块将对该数据报执行一些操作， 然后将它交给IP数据报输出子模块。 我们将在后面讨论数据报转发的具体过程。
      > IP数据报应该发送至哪个下一跳路由（或者目标机器） ， 以及经过哪个网卡来发送， 就是IP路由过程，即图2-3中“计算下一跳路由”子模块。 IP模块实现数据报路由的核心数据结构是路由表。 这个表按照数据报的目标IP地址分类， 同一类型的IP数据报将被发往相同的下一跳路由器（或者目标机器） 。 我们将在后面讨论IP路由过程。
      > IP输出队列中存放的是所有等待发送的IP数据报， 其中除了需要转发的IP数据报外， 还包括封装了本机上层数据（ICMP报文、 TCP报文段和UDP数据报） 的IP数据报。
      > 图2-3中的虚线箭头显示了路由表更新的过程。 这一过程是指通过路由协议或者route命令调整路由表，使之更适应最新的网络拓扑结构， 称为IP路由策略。 我们将在后面简单讨论它。  

5. ICMP重定向

6. IPV6头部结构

### 第三章：TCP协议详解：

1. TCP服务的特点

   1. 主要特点：面向连接、字节流、提供可靠传输
   2. 字节流的概念：发送端的写操作和接收端的读操作的次数之间没有任何关系，当发送端应用程序连续执行多次写操作是，TCP模块会先将数据放到发送缓冲区，当TCP模块真的开始发送数据时，缓冲区的数据会被封装成一个或者多个TCP报文发送出去，因此，发送端的TCP包的数量和写操作没有关系，同理接收端也没有任何关系
   3. TCP字节流和UDP数据报：![image-20210306153555121](https://github.com/zb1997/mynote/blob/main/tupian/image-20210306153555121.png)

2. TCP头部结构

3. TCP连接的建立和关闭

   1. 三次握手和四次挥手
   2. 半关闭状态
   3. 连接超时

4. TCP状态转移

   1. 状态转移图![image-20210306153831889](https://github.com/zb1997/mynote/blob/main/tupian/image-20210306153831889.png)
      ![image-20210306153904961](https://github.com/zb1997/mynote/blob/main/tupian/image-20210306153904961.png)
   2. TIME_WAIT状态：在客户端收到服务端的结束报文之后，会等待两个MSL(报文最大生存时间)才会完全关闭，理由：
      1. 可靠的终止TCP连接：当报文7丢失时，服务端会重发报文6，客户端需要继续对这个报文回复，否则客户端将以复位报文段来回应服务器，服务器则认为这是一个错误，因为他期望的是一个像TCP报文7一样的确认报文
      2. 保证让迟到的报文段有足够的事件被识别并被丢弃：在linux系统上，一个TCP端口不能被同时多次打开，当一个TCP连接处于TIME_WAIT状态时，我们将无法立即使用改连接占用着的端口里建立一个新的连接，如果不存在TIME_WAIT状态，则应用程序能够立即建立一个和原来一样的连接，这个连接就可能会受到之前的数据包，这显然是不应该发生的
      3. 2个MSL的原因是TIME_WAIT状态能够确保网络上的两个传输方向上的未被接受的、迟到的报文段都已经消失，所以一个新的连接在2MSL时间后可以安全的建立

5. 复位报文段：特殊情况下，TCP连接的一端会向对方发送带RST标志的报文段，通知对方关闭连接或者重新建立连接，谈论其中的三种情况

   1. 访问不存在的端口或者端口处于TIME_WAIT状态
   2. 异常终止连接：TCP提供异常终止连接的方法，即给对方发送一个复位报文段，一端发送了复位报文段，发送端所有等待发送的数据都会被丢弃，应用程序可以使用socket选项SO_LINGER来发送复位报文段
   3. 处理板打开连接：如果TCP连接的一端由于某种原因没有发送结束报文段或者结束报文段由于网络原因没有发送二终止了连接，那么另外一端就处于半打开状态，当半打开状态的连接发送数据时，对方将会恢复一个复位报文段

6. TCP交互数据

7. TCP成块数据

8. 带外数据(Out Of Band)

9. TCP超时重传

   1. tcp重传策略：TCP一共执行5次重传。每次重传的时间都增加一倍，在重传失败的情况下，底层的IP和ARP开始接管连接，Linux有两个重要的内核参数与TCP超时重传相关： /proc/sys/net/ipv4/tcp_retries1和/proc/sys/net/ipv4/tcp_retries2。 前者指定在底层IP接管之前TCP最少执行的重传次数， 默认值是3。 后者指定连接放弃前TCP最多可以执行的重传次数， 默认值是15（一般对应13～30 min）   
   2. 虽然超时会导致重传，但是重传可以发生在超时之前，即快速重传

10. 拥塞控制   慢开始 拥塞避免 快重传 快恢复

    ​	https://github.com/CyC2018/CS-Notes/blob/master/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%20-%20%E4%BC%A0%E8%BE%93%E5%B1%82.md
    
    这篇文章中有比较形象的说明

### 第四章：TCP／IP通信案例

。。。

## 第二篇：深入解析高性能服务器编程

### 第五章：linux网络编程基础API

1. socket地址api：
   1. 主机序(小端)和网络序(大端)
   2. 通用sockaddr   结构体sockaddr
   3. 专用sockaddr   结构体sockaddr_un sockaddr_in sockaddr_in6
   4. IP地址转换函数
   
2. 创建socket、命名、监听: create bind listen

3. 连接相关：接受、发起、关闭 accept 

4. 数据读写 send recv

   上面三个里面涉及很多的标志位，能够产生不同的效果

5. 带外标记

6. 地址信息函数

7. scoket选项 setsockopt getsockopt 选项对应的功能比较重要
   ![image-20210308192134473](https://github.com/zb1997/mynote/blob/main/tupian/image-20210308192134473.png)

8. 网络信息api

### 第六章：高级IO函数：

​	分为三类：	

​		用于创建文件描述符的函数，包括pip、dup、dup2  

​		用于读写数据：readv、writev、sendfile、mmap、munmap、splice、tee

​		用于控制IO行为和属性：fcntl

1. pip函数：用于创建一个管道，实现进程间通信

   1. 管道内部传输的数据是字节流， 这和TCP字节流的概念相同。 但二者又有细微的区别。 应用层程序能往一个TCP连接中写入多少字节的数据， 取决于对方的接收通告窗口的大小和本端的拥塞窗口的大小。 而管道本身拥有一个容量限制， 它规定如果应用程序不将数据从管道读走的话， 该管道最多能被写入多少字节的数据。 自Linux 2.6.11内核起，管道容量的大小默认是65536字节。 我们可以使用fcntl函数来修改管道容量  

2. dup函数：创建一个新的文件描述符，他和原有描述符只想相同的文件、管道或者网络连接，并且dup返回的文件描述符总是取当前系统当前可用的最小值。dup2和dup相似，但是他返回第一个不小于原描述符的整数值

3. readv和writev函数：readv函数将数据从文件描述符读到分散的内存块中，writev函数将多块分散的内存数据一并写入文件描述符中。分散读和集中写

4. sendfile函数：在两个描述符之间直接传递数据(完全在内核中操作)，从而避免了内核缓冲区和用户缓冲区之间的数据拷贝，效率很高，被称为零拷贝。

   该函数的man手册明确指出，in_fd必须是一个支持类似mmap函数的文件描述符，即他必须指向真实的文件，不能是socket或者管道。而out_fd一定是scoket。由此可见，sendfile几乎是专为在网络上传输文件为设计的

5. mmap和munmap函数：用于申请一段内存空间，我们可以将这段内存作为进程间通信的共享内存，而munmap函数则是释放这段内存空间。

6. splice函数：用于在两个文件描述符之间移动数据，也是零拷贝操作。使用splice函数时， fd_in和fd_out必须至少有一个是**管道**文件描述符  

7. tee函数：在两个管道文件描述符之间复制数据，也是零拷贝操作。他不消耗数据，因此源文件描述符上为数据仍然可以用于后续的读操作

8. fcntl函数：filecontrol，提供了对文件描述符的各种控制操作。另外一个常见的控制文件描述符属性和行为的系统调用是ioctl。而且ioctl比fcntl能够执行更多的控制，但是对于文件描述符常用的属性和行为，fcntl函数是由POSIX规范制定的首选方法。在网络编程中，fnctl经常被用来设置非阻塞标志

### 第七章：linux服务器程序规范：

- linux服务器程序一般以守护进程形式运行，没有控制终端，因而也不会意外收到用户输入，守护进程的父进程通常是init进程(PID = 1)
- Linux服务器程序通常有一套日志系统， 它至少能输出日志到文件， 有的高级服务器还能输出日志到专门的UDP服务器。 大部分后台进程都在/var/log目录下拥有自己的日志目录。
- Linux服务器程序一般以某个专门的非root身份运行。 比如mysqld、 httpd、 syslogd等后台进程， 分别拥有自己的运行账户mysql、 apache和syslog。
- Linux服务器程序通常是可配置的。 服务器程序通常能处理很多命令行选项， 如果一次运行的选项太多， 则可以用配置文件来管理。 绝大多数服务器程序都有配置文件， 并存放在/etc目录下。 比如第4章讨论的squid服务器的配置文件是/etc/squid3/squid.conf。
- Linux服务器进程通常会在启动的时候生成一个PID文件并存入/var/run目录中， 以记录该后台进程的PID。 比如syslogd的PID文件是/var/run/syslogd.pid  
- Linux服务器程序通常需要考虑系统资源和限制， 以预测自身能承受多大负荷， 比如进程可用文件描述符总数和内存总量等。  

1. 日志
   1. linux系统日志：linux提供一个守护进程来处理系统日志--rsyslogd
      1. rsyslogd守护进程既能接收用户进程输出的日志，也能接收内内核日志。用户进程通过调用syslog函数生成系统日志，该函数将日志输出到一个UNIX本地域socket类型的文件/dev/log中，rsyslogd则监听该文件获取进程的输出。
      2. 内核日志在老的系统上是通过另外一个守护进程rklogd来管理的， rsyslogd利用额外的模块实现了相同的功能。 内核日志由printk等函数打印至内核的环状缓存（ringbuffer） 中。 环状缓存的内容直接映射到/proc/kmsg文件中。 rsyslogd则通过读取该文件获得内核日志。  
      3. rsyslogd守护进程在接收到用户进程或内核输入的日志后， 会把它们输出至某些特定的日志文件。 默认情况下， 调试信息会保存至/var/log/debug文件， 普通信息保存至/var/log/messages文件， 内核消息则保存至/var/log/kern.log文件。 不过， 日志信息具体如何分发， 可以在rsyslogd的配置文件中设置。 rsyslogd的主配置文件是/etc/rsyslog.conf， 其中主要可以设置的项包括： 内核日志输入路径， 是否接收UDP日志及其监听端口（默认是514， 见/etc/services文件） ， 是否接收TCP日志及其监听端口， 日志文件的权限， 包含哪些子配置文件（比如/etc/rsyslog.d/*.conf） 。 rsyslogd的子配置文件则指定各类日志的目标存储文件  
      4. ![image-20210308205139378](https://github.com/zb1997/mynote/blob/main/tupian/image-20210308205139378.png)
   2. syslog函数:日志掩码
2. 用户信息
   1. 真实用户UID、有效用户EUID、真实组GID、有效组EGID
      1. EUID存在的目的是方便用来进行资源访问，使得运行程序的用户拥有该程序的有效权限。EGID也是一样的意义
   2. 切换用户
3. 进程间关系
   1. 进程组：linux每个进程都属于一个进程组，因此他们除了PID外，还有进程组ID。一个进程只能对设置自己或者子进程的PGID
   2. 会话session
   3. 用ps命令查看进程关系：
4. 系统资源限制：linux上运行的程序都会受到资源限制的影响，比如物理设备、系统策略、和具体实现的限制。
5. 改变工作目录和根目录
6. 服务器程序后台化：守护进程![image-20210310200127158](https://github.com/zb1997/mynote/blob/main/tupian/image-20210310200127158.png)

### 第八章：高性能服务器程序框架

**全书的核心**

将服务器解构为如下三个主要模块：

- **IO处理单元**：介绍IO处理单元的四种IO模型和两种高效事件处理模式
- **逻辑单元**：介绍逻辑单元两种高效并发模式，以及高效的逻辑处理方式：有限状态机
- 存储单元

1. ##### 服务器模型：

   1. ###### **C/S模型**：服务器端一般通过IO模型监听事件请求，在监听到连接请求之后，服务器调用accept函数接受它，并分配一个逻辑单元为新的连接服务。逻辑单元可以是新创建的子进程、子线程、或者其他。

   2. **P2P模型**：摒弃了以服务器为中心的格局，让网络所有主机重新回归对等的地位，使得每台机器都在消耗服务的同时也为其他人提供服务。云计算机群可以看做P2P模型的一个典范。缺点很明显，当用户之间传输的请求过多时，网络的负载将加重，并且主机之间很难互相发现。

2. **服务器编程框架**:
   
   1. ![image-20210310202814160](https://github.com/zb1997/mynote/blob/main/tupian/image-20210310202814160.png)
   2. 注：请求队列是各单元之间的通信方式的抽象。 I/O处理单元接收到客户请求时， 需要以某种方式通知一个逻辑单元来处理该请求。 同样， 多个逻辑单元同时访问一个存储单元时， 也需要采用某种机制来协调处理竞态条件。 请求队列通常被实现为池的一部分， 我们将在后面讨论池的概念。 对于服务器机群而言， **请求队列是各台服务器之间预先建立的、 静态的、 永久的TCP连接**。 这种TCP连接能提高服务器之间交换数据的效率， 因为它避免了动态建立TCP连接导致的额外的系统开销。  
   
3. **IO模型**：
   
   1. **阻塞IO和非阻塞IO**：可能被阻塞的系统调用有：accept、send、recv、connect
      1. 针对非阻塞I/O执行的系统调用则总是立即返回， 而不管事件是否已经发生。 如果事件没有立即发生，这些系统调用就返回-1， 和出错的情况一样。 此时我们必须根据errno来区分这两种情况。对accept、 send和recv而言， 事件未发生时errno通常被设置成EAGAIN（意为“再来一次”） 或者EWOULDBLOCK（意为“期望阻塞”） ； 对connect而言， errno则被设置成EINPROGRESS（意为“在处理中 ")
      2. 非阻塞I/O通常要和其他I/O通知机制一起使用， 比如**I/O复用和SIGIO信号**。 
         1.  **I/O复用**是最常使用的I/O通知机制。 它指的是， 应用程序通过I/O复用函数向内核注册一组事件， 内核通过I/O复用函数把其中就绪的事件通知给应用程序。 Linux上常用的I/O复用函数是select、 poll和epoll_wait，需要指出的是， **I/O复用函数本身是阻塞的**， 它们能提高程序效率的原因在于它们具有同时监听多个I/O事件的能力。  
         2. SIGIO信号也可以用来报告I/O事件。 我们可以为一个目标文件描述符指定宿主进程， 那么被指定的宿主进程将捕获到SIGIO信号。 这样， 当目标文件描述符上有事件发生时， SIGIO信号的信号处理函数将被触发， 我们也就可以在该信号处理函数中对目标文件描述符执行非阻塞I/O操作了。 
   2. 从理论上说， **阻塞I/O、 I/O复用和信号驱动I/O都是同步I/O模型**。 因为在这三种I/O模型中， I/O的读写操作， 都是在I/O事件发生之后， 由应用程序来完成的。 而POSIX规范所定义的**异步I/O模型**则不同。 对异步I/O而言， 用户可以直接对I/O执行读写操作， 这些操作告诉内核用户读写缓冲区的位置， 以及I/O操作完成之后内核通知应用程序的方式。 异步I/O的读写操作总是立即返回， 而不论I/O是否是阻塞的， 因为真正的读写操作已经由内核接管。 也就是说， **同步I/O模型要求用户代码自行执行I/O操作（将数据从内核缓冲区读入用户缓冲区， 或将数据从用户缓冲区写入内核缓冲区） ， 而异步I/O机制则由内核来执行I/O操作（数据在内核缓冲区 和用户缓冲区之间的移动是由内核在“后台”完成的）** 。 你可以这样认为， **同步I/O向应用程序通知的是I/O就绪事件， 而异步I/O向应用程序通知的是I/O完成事件**。 Linux环境下， aio.h头文件中定义的函数提供了对异步I/O的支持。   
   3. ![image-20210310210717168](https://github.com/zb1997/mynote/blob/main/tupian/image-20210310210717168.png)
   
4. 两种高效的**事件处理模式**：同步IO模型通常用于实现REACTOR模式，异步IO则用于实现PROACTOR

   1. **REACTOR**：

      1. > Reactor是这样一种模式， 它要求主线程（I/O处理单元， 下同） 只负责监听文件描述上是否有事件发生， 有的话就立即将该事件通知工作线程（逻辑单元， 下同） 。 除此之外， 主线程不做任何其他实质性的工作。 读写数据， 接受新的连接， 以及处理客户请求均在工作线程中完成。
         > 使用同步I/O模型（以epoll_wait为例） 实现的Reactor模式的工作流程是：
         > 1） 主线程往epoll内核事件表中注册socket上的读就绪事件。
         > 2） 主线程调用epoll_wait等待socket上有数据可读。
         > 3） 当socket上有数据可读时， epoll_wait通知主线程。 主线程则将socket可读事件放入请求队列。
         > 4） 睡眠在请求队列上的某个工作线程被唤醒， 它从socket读取数据， 并处理客户请求， 然后往epoll内核事件表中注册该socket上的写就绪事件。
         > 5） 主线程调用epoll_wait等待socket可写。
         > 6） 当socket可写时， epoll_wait通知主线程。 主线程将socket可写事件放入请求队列。
         > 7） 睡眠在请求队列上的某个工作线程被唤醒， 它往socket上写入服务器处理客户请求的结果。  

      2. ![image-20210311192641675](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311192641675.png)
         工作线程从请求队列中取出事件后， 将根据事件的类型来决定如何处理它： 对于可读事件，执行读数据和处理请求的操作； 对于可写事件， 执行写数据的操作。 因此， 图8-5所示的Reactor模式中， 没必要区分所谓的“读工作线程”和“写工作线程”。  

   2. **PROACTOR**：

      1. > 与Reactor模式不同， Proactor模式将所有I/O操作都交给主线程和内核来处理， 工作线程仅仅负责业务逻辑。 因此， Proactor模式更符合图8-4所描述的服务器编程框架。使用异步I/O模型（以aio_read和aio_write为例） 实现的Proactor模式的工作流程是：
         > 1） 主线程调用aio_read函数向内核注册socket上的读完成事件， 并告诉内核用户读缓冲区的位置， 以及读操作完成时如何通知应用程序（这里以信号为例， 详情请参考sigevent的man手册） 。
         > 2） 主线程继续处理其他逻辑。
         > 3） 当socket上的数据被读入用户缓冲区后， 内核将向应用程序发送一个信号， 以通知应用程序数据已经可用。
         > 4） 应用程序预先定义好的信号处理函数选择一个工作线程来处理客户请求。 工作线程处理完客户请求之后， 调用aio_write函数向内核注册socket上的写完成事件， 告诉内核用户写缓冲区的位置， 以及写操作完成时如何通知应用程序（仍然以信号为例） 。
         > 5） 主线程继续处理其他逻辑。
         > 6） 当用户缓冲区的数据被写入socket之后， 内核将向应用程序发送一个信号， 以通知应用程序数据已经发送完毕。
         > 7） 应用程序预先定义好的信号处理函数选择一个工作线程来做善后处理， 比如决定是否关闭socket。  

      2. ![image-20210311194012372](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311194012372.png)
         连接socket上的读写事件是通过aio_read/aio_write向内核注册的， 因此内核将通过信号来向应用程序报告连接socket上的读写事件。 所以， 主线程中的epoll_wait调用仅能用来检测监听socket上的连接请求事件， 而不能用来检测连接socket上的读写事件。  

      3. 模拟Proactor模式：使用同步IO方式模拟出PROACTOR模式。原理是:主线程执行数据读写操作，读写完成之后，主线程向工作线程通知这一完成事件，那么从工作线程的角度看，他们就直接获得了数据读写的结果，接下来要做的只是对读写结果进行逻辑处理

         > 使用同步I/O模型（仍然以epoll_wait为例） 模拟出的Proactor模式的工作流程如下：
         > 1） 主线程往epoll内核事件表中注册socket上的读就绪事件。
         > 2） 主线程调用epoll_wait等待socket上有数据可读。
         > 3） 当socket上有数据可读时， epoll_wait通知主线程。 主线程从socket循环读取数据， 直到没有更多数据可读， 然后将读取到的数据封装成一个请求对象并插入请求队列。
         > 4） 睡眠在请求队列上的某个工作线程被唤醒， 它获得请求对象并处理客户请求， 然后往epoll内核事件表中注册socket上的写就绪事件。
         > 5） 主线程调用epoll_wait等待socket可写。
         > 6） 当socket可写时， epoll_wait通知主线程。 主线程往socket上写入服务器处理客户请求的结果  

         ![image-20210311194439760](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311194439760.png)

5. 两种高效的**并发模式**：半同步/半异步模式(half-sync/half-async)和领导者/追随者模式(Leader/Followers)

   1. **半同步半异步模式**：

      1. 同步与异步:
         1. IO模型中的同步异步区分的是内核向应用程序通知的是何种IO事件(就绪或者完成)，以及谁来完成IO读写(应用程序或者是内核)。
         2. 并发模型中的同步是值程序完全按照代码序列的顺序执行，异步指的是程序的执行那个需要由系统事件来驱动。常见的系统事件包括终端、信号等
         3. ![image-20210311195348494](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311195352826.png)
         4. 同步并发模式：效率相对较低，实时性差，但是逻辑简单
            异步并发模式：效率高、实时性强、但是相对复杂，难于调试和扩展
      2. 变体：**半同步半反应堆模式**：![image-20210311200343287](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311200343287.png)
         1. 缺点：主线程和工作线程共享请求队列，读写都需要枷锁保护，白白消耗CPU时间
         2. 每个工作线程一个事件只能处理一个客户请求，如果客户数量很多，客户的响应速度也会变慢，如果增加工作线程，则工作线程的切换也会耗费大量的CPU时间
      3. 相对高效的半同步半异步模式：![image-20210311200840960](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311200840960.png)
         1. 主线程只管监听socket，连接socket由工作线程来管理。当有新的连接到来，主线程就接收并将新返回的socket派发给某个工作线程，此后的所有操作都归工作线程管理。主线程向工作线程派发scoket最简单的方式是往他和工作线程之间的管道写数据，工作线程检测到管道中的数据。

   2. **领导者/追随者模式**：

      1. 多个工作线程轮流获得事件源集合， 轮流监听、 分发并处理事件的一种模式。 在任意时间点，程序都仅有一个领导者线程， 它负责监听I/O事件。 而其他线程则都是追随者， 它们休眠在线程池中等待成为新的领导者。 当前的领导者如果检测到I/O事件， 首先要从线程池中推选出新的领导者线程， 然后处理I/O事件。 此时， 新的领导者等待新的I/O事件， 而原来的领导者则处理I/O事件， 二者实现了并发。  

      2. 包含组件：句柄集、线程集、事件处理器和具体事件处理器![image-20210311201656702](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311201656702.png)

         1. 句柄集：表示I/O资源， 在Linux下通常就是一个文件描述符。 句柄集管理众多句柄， 它使用
            wait_for_event方法来监听这些句柄上的I/O事件， 并将其中的就绪事件通知给领导者线程。 领导者则调用绑定到Handle上的事件处理器来处理事件。 领导者将Handle和事件处理器绑定是通过调用句柄集中的register_handle方法实现的。  

         2. 线程集：是所有工作线程（包括领导者线程和追随者线程） 的管理者。 它负责各线程之间的同步， 以及新领导者线程的推选。 线程集中的线程在任一时间必处于如下三种状态之一：

            - Leader： 线程当前处于领导者身份， 负责等待句柄集上的I/O事件。

            - Processing： 线程正在处理事件。 领导者检测到I/O事件之后， 可以转移到Processing状态来处理该事件， 并调用promote_new_leader方法推选新的领导者； 也可以指定其他追随者来处理事件（EventHandoff） ， 此时领导者的地位不变。 当处于Processing状态的线程处理完事件之后， 如果当前线程集中没有领导者， 则它将成为新的领导者， 否则它就直接转变为追随者。

            - Follower： 线程当前处于追随者身份， 通过调用线程集的join方法等待成为新的领导者， 也可能被当前的领导者指定来处理新的任务  

              状态转换关系：

              ![image-20210311202637060](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311202637060.png)

         3. 事件处理器和具体事件处理器

            1. 通常包含一个或多个回调函数handle_event。 这些回调函数用于处理事件对应的业务逻辑。事件处理器在使用前需要被绑定到某个句柄上， 当该句柄上有事件发生时， 领导者就执行与之绑定的事件处理器中的回调函数。  
            2. 工作流程：![image-20210311203049785](https://github.com/zb1997/mynote/blob/main/tupian/image-20210311203049785.png)

6.   **有限状态机**：逻辑单元内的高效编程方法

7. 提高服务器性能的其他建议：

   1. 池：由于服务器基本上硬件资源充足，所以可以使用以空间换时间的方案，这就是此的概念。池是一组资源的集合，在服务器启动的时候就静态分配好，无需动态分配。释放时只需要将资源还给池中，无需执行释放操作。可以避免很多服务器对内核的操作

      1. 两种分配方案：分配足够多的资源和预分配一定数量的资源，之后发现不够在进行动态分配
      2. 内存池 进程池 线程池 连接池

   2. 数据复制：避免不必要的数据复制尤其是当数据复制发生在用户代码和内核之间的时候。 如
      果内核可以直接处理从socket或者文件读入的数据， 则应用程序就没必要将这些数据从内核缓冲区制到应用程序缓冲区中。 这里说的“直接处理”指的是应用程序不关心这些数据的内容， 不需要对它们做任何分析。 比如ftp服务器， 当客户请求一个文件时， 服务器只需要检测目标文件是否存在， 以及客户是否有读取它的权限， 而绝对不会关心文件的具体内容。 这样的话， **ftp服务器就无须把目标文件的内容完整地读入到应用程序缓冲区中并调用send函数来发送， 而是可以使用“零拷贝”函数sendfile来直接将其发送给客户端**  

   3. 上下文切换和锁：

      > 并发程序必须考虑上下文切换（context switch） 的问题， 即进程切换或线程切换导致的的系统开销。即使是I/O密集型的服务器， 也不应该使用过多的工作线程（或工作进程， 下同） ， 否则线程间的切换将占用大量的CPU时间， 服务器真正用于处理业务逻辑的CPU时间的比重就显得不足了。 因此， 为每个客户连接都创建一个工作线程的服务器模型是不可取的。 图8-11所描述的半同步/半异步模式是一种比较合理的解决方案， 它允许一个线程同时处理多个客户连接。 此外， 多线程服务器的一个优点是不同的线程可以同时运行在不同的CPU上。 当线程的数量不大于CPU的数目时， 上下文的切换就不是问题了。
      > 并发程序需要考虑的另外一个问题是共享资源的加锁保护。 锁通常被认为是导致服务器效率低下的一个因素， 因为由它引入的代码不仅不处理任何业务逻辑， 而且需要访问内核资源。 因此， 服务器如果有更好的解决方案， 就应该避免使用锁。 显然， 图8-11所描述的半同步/半异步模式就比图8-10所描述的半同步/半反应堆模式的效率高。 如果服务器必须使用“锁”， 则可以考虑减小锁的粒度， 比如使用读写锁。 当所有工作线程都只读取一块共享内存的内容时， 读写锁并不会增加系统的额外开销。 只有当其中某一个工作线程需要写这块内存时， 系统才必须去锁住这块区域。  

